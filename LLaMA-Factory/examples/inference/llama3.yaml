model_name_or_path: /models/Meta-Llama-3.1-8B-Instruct
template: llama3
flash_attn: fa2
infer_backend: vllm
vllm_maxlen: 32768
vllm_enforce_eager: true
#adapter_path=
#vllm_gpu_util: 0.85